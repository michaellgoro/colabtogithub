{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aml2022_report.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rPWwlW4F0P0i"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michaellgoro/colabtogithub/blob/master/aml2022_report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Machine Learning (2022) Final Report Assignment\n",
        "\n",
        "Answer Questions 1 to 4 (either in Japanese or English). Submit a report in either PDF (.pdf) or JupyterNotebook (.ipynb) format.\n",
        "\n",
        "## Question 1 (50 points)\n",
        "\n",
        "Consider a convolutional neural network (CNN) that predicts a label $\\hat{y} \\in \\{0, 1\\}$ for a given sentence $\\boldsymbol{X} \\in \\mathbb{R}^{d \\times T}$. Here, a sentence is represented by a matrix $\\boldsymbol{X} = (\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_T)$ consisting of a concatenation of $T$ word embeddings, $\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_T \\in \\mathbb{R}^d$, where $d$ is the size of word embeddings, and $T$ is the number of words in the sentence.\n",
        "\n",
        "These equations define the whole architecture of the CNN.\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y} &= \\begin{cases}\n",
        "1 & (0.5 < p) \\\\\n",
        "0 & (p \\leq 0.5)\n",
        "\\end{cases} \\\\\n",
        "p &= \\sigma(\\boldsymbol{v}^\\top \\boldsymbol{s}) \\\\\n",
        "\\boldsymbol{s} &= \\max(\\boldsymbol{c}_1, \\dots, \\boldsymbol{c}_{T-\\delta+1}) \\\\\n",
        "\\boldsymbol{c}_t &= {\\rm ReLU}(\\boldsymbol{W} \\boldsymbol{x}_{t:t+\\delta-1} + \\boldsymbol{b}) & (\\forall t \\in \\{1, \\dots, T-\\delta+1\\}) \\\\\n",
        "\\boldsymbol{x}_{t:t+\\delta-1} &= \\boldsymbol{x}_{t} \\oplus \\boldsymbol{x}_{t+1} \\oplus \\dots \\oplus \\boldsymbol{x}_{t+\\delta-1}\n",
        "\\end{align}\n",
        "\n",
        "Here:\n",
        "\n",
        "+ $\\boldsymbol{W} \\in \\mathbb{R}^{m \\times \\delta d}$, $\\boldsymbol{b} \\in \\mathbb{R}^m, \\boldsymbol{v} \\in \\mathbb{R}^m$ are the model parameters;\n",
        "+ $m$ denotes the number of output channels of the CNN;\n",
        "+ $\\delta$ denotes the width (kernel size) of the convolution;\n",
        "+ $\\sigma(\\cdot)$ denotes the standard sigmoid function;\n",
        "+ $\\max(\\cdot)$ presents the max pooling operation;\n",
        "+ ${\\rm ReLU}(\\cdot)$ denotes the ReLU activation function;\n",
        "+ $\\oplus$ presents a concatenation of vectors.\n",
        "\n",
        "Setting the hyperparameters $d=3, m=2, \\delta=2$, we initialize the model parameters as follows.\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{W} &= \\begin{pmatrix}\n",
        "-3 & -2 & -1 & -1 & -2 & -3 \\\\\n",
        "3 & 2 & 3 & 2 & 3 & 2\n",
        "\\end{pmatrix} \\\\\n",
        "\\boldsymbol{b} &= \\begin{pmatrix}\n",
        "-0.2 \\\\ 0.1\n",
        "\\end{pmatrix} \\\\\n",
        "\\boldsymbol{v} &= \\begin{pmatrix}\n",
        "-1 \\\\ 2\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Suppose that we give a negative ($y=0$) training instance with the sentence ($T = 5$),\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{X} &= \\begin{pmatrix}\n",
        "-0.3 & 0 & 0.1 & 0 & 0 \\\\\n",
        "-0.2 & -0.1 & 0 & 0.1 & 0 \\\\\n",
        "-0.1 & -0.2 & 0.1 & 0 & 0.1\n",
        "\\end{pmatrix} ,\n",
        "\\end{align}\n",
        "to the CNN model, and answer the following questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "-z-I39fBi1MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.array([[-0.3, 0, 0.1, 0,0],\n",
        "              [-0.2, -0.1, 0, 0.1, 0],\n",
        "              [-0.1, -0.2, 0.1, 0, 0.1]])\n",
        "\n",
        "W = np.array([[-3, -2, -1, -1, -2, -3],\n",
        "              [3, 2, 3, 2, 3, 2]])\n",
        "b = np.array([-0.2, 0.1])\n",
        "v = np.array([-1, 2])\n",
        "\n"
      ],
      "metadata": {
        "id": "3S9I5NB9Pf0q"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(1)** Find the value of the vector $\\boldsymbol{x}_{3:4}$.\n",
        "\n",
        "$x_{3:4}=(0.1,0,0.1,0,0.1,0)^T$"
      ],
      "metadata": {
        "id": "hqXUJrLRJm6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "x_34 = np.ravel(X[:, 2:4])\n",
        "x_34"
      ],
      "metadata": {
        "id": "uec36GNAyNdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a653342-d152-4130-9300-989f801dec17"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.1, 0. , 0. , 0.1, 0.1, 0. ])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(2)** Find the values of the hidden vectors $\\boldsymbol{c}_1, \\boldsymbol{c}_2, \\boldsymbol{c}_3, \\boldsymbol{c}_4$."
      ],
      "metadata": {
        "id": "7XCf-KoPyOfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#c1~c4を格納するためのlist\n",
        "# W*x_34\n",
        "c = []\n",
        "print(W)\n",
        "for i in range(4):\n",
        "  x = np.ravel(X[:, i:i+2])\n",
        "  # print(x.shape)\n",
        "  print(x)\n",
        "  c.append(W@x+b)\n",
        "print()\n",
        "c = np.array(c).T\n",
        "print(c)\n",
        "for i in range(len(c)):\n",
        "  for j in range(len(c[0])):\n",
        "    if c[i][j] < 0:\n",
        "      c[i][j] = 0\n",
        "print(c)\n",
        "# for i in range(4):\n",
        "#   print(f\"c{i+1}\")\n",
        "#   print(c[i])\n",
        "#   print()"
      ],
      "metadata": {
        "id": "sgxmns3JyQgB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfad594-cdce-4957-b5f0-672955f6f44c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-3 -2 -1 -1 -2 -3]\n",
            " [ 3  2  3  2  3  2]]\n",
            "[-0.3  0.  -0.2 -0.1 -0.1 -0.2]\n",
            "[ 0.   0.1 -0.1  0.  -0.2  0.1]\n",
            "[0.1 0.  0.  0.1 0.1 0. ]\n",
            "[0.  0.  0.1 0.  0.  0.1]\n",
            "\n",
            "[[ 1.8 -0.2 -0.8 -0.6]\n",
            " [-2.3 -0.4  0.9  0.6]]\n",
            "[[1.8 0.  0.  0. ]\n",
            " [0.  0.  0.9 0.6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{align}\n",
        "\\boldsymbol{c}_1 &= \\begin{pmatrix}\n",
        "1.8\\\\\n",
        "0 \\\\\n",
        "\\end{pmatrix} \\\\\n",
        "\\boldsymbol{c}_2 &= \\begin{pmatrix}\n",
        "0\\\\\n",
        "0 \\\\\n",
        "\\end{pmatrix}  \\\\\n",
        "\\boldsymbol{c}_3 &= \\begin{pmatrix}\n",
        "0 \\\\\n",
        "0.9 \\\\\n",
        "\\end{pmatrix}  \\\\\n",
        "\\boldsymbol{c}_4 &= \\begin{pmatrix}\n",
        "0 \\\\\n",
        "0.6 \\\\\n",
        "\\end{pmatrix}  \n",
        "\\end{align}"
      ],
      "metadata": {
        "id": "nJGS9AehTtk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(3)** Find the value of the vector $\\boldsymbol{s}$.\n"
      ],
      "metadata": {
        "id": "kVFqVPWPyQ58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s = c.max(axis=1)\n",
        "s"
      ],
      "metadata": {
        "id": "Vt3UQqqAyTVj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10e892c4-3d64-4771-87f9-5dc811011d22"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.8, 0.9])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\boldsymbol{s} = (1.8, 0.9)^T$"
      ],
      "metadata": {
        "id": "-6geMN7yWWms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(4)** Find the value of $p$."
      ],
      "metadata": {
        "id": "Kt26e_TSyT6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(a):\n",
        "  return 1 / (1 + np.exp(-a))\n",
        "p = sigmoid(v @ s)\n",
        "p"
      ],
      "metadata": {
        "id": "U4g6j4L3y635",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9030c50d-e42a-45e2-b959-0152a27eeb92"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(5)** Write the formula of the binary cross-entropy loss between the correct label $y$ and the probability estimate $p$."
      ],
      "metadata": {
        "id": "fUh6ZlAqyWRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$l(p,y) = -y\\log{p}-(1-y)\\log{(1-p)}$"
      ],
      "metadata": {
        "id": "hGYnq54vi0SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(6)** Compute the loss value by using the formula of (5) for the training instance."
      ],
      "metadata": {
        "id": "p9WQToWfyYuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = -1*np.log(sigmoid(p))-\n",
        "def calc_loss(p,y):\n",
        "  print(np.log(sigmoid(p)))\n",
        "  loss = -y * np.log(p) - (1 - y) * np.log(1-p)\n",
        "  return loss\n",
        "calc_loss(p,1)"
      ],
      "metadata": {
        "id": "LsFTKdK3zpYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f07ea061-338a-4e8b-9a34-ec2354256a99"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-0.47407698418010663\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6931471805599453"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(7)** Compute the gradient of the loss function with respect to $\\boldsymbol{v}$ for the training instance."
      ],
      "metadata": {
        "id": "6b03t2amzpz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "dtype = torch.float\n",
        "st = torch.from_numpy(s.astype(np.float32)).clone()\n",
        "v = torch.tensor([-1, 2], dtype=dtype, requires_grad=True)\n",
        "print(st,v)\n",
        "def sigmoid_tensor(a):\n",
        "  return 1 / (1+torch.exp(-a))\n",
        "pt = sigmoid_tensor(torch.dot(st,v))\n",
        "print(pt)\n",
        "\n",
        "def calc_loss_tensor(p,y):\n",
        "  print(torch.log(sigmoid_tensor(p)))\n",
        "  print(sigmoid_tensor(1-p))\n",
        "  loss = -y * torch.log(p) - (1 - y) * torch.log(1-p)\n",
        "  return loss\n",
        "loss = calc_loss_tensor(pt,1)\n",
        "print(loss)\n",
        "loss.backward()\n",
        "v.grad"
      ],
      "metadata": {
        "id": "Oa0xGZNgz7bY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "309ff8f1-9959-44ff-97f8-49832bfc79af"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.8000, 0.9000]) tensor([-1.,  2.], requires_grad=True)\n",
            "tensor(0.5000, grad_fn=<MulBackward0>)\n",
            "tensor(-0.4741, grad_fn=<LogBackward0>)\n",
            "tensor(0.6225, grad_fn=<MulBackward0>)\n",
            "tensor(0.6931, grad_fn=<SubBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.9000, -0.4500])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(8)** Compute the gradients of the loss function with respect to $\\boldsymbol{W}$ for the training instance."
      ],
      "metadata": {
        "id": "CTt2o7hKz7zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "W = torch.tensor([[-3, -2, -1, -1, -2, -3],\n",
        "              [3, 2, 3, 2, 3, 2]], dtype=dtype, requires_grad=True)\n",
        "X = torch.tensor([[-0.3, 0, 0.1, 0,0],\n",
        "              [-0.2, -0.1, 0, 0.1, 0],\n",
        "              [-0.1, -0.2, 0.1, 0, 0.1]], dtype=dtype)\n",
        "b =torch.tensor([-0.2, 0.1],dtype=dtype)\n",
        "v = torch.tensor([-1, 2],dtype=dtype)\n",
        "trans_X = X[:, 0:2].reshape(6, -1)\n",
        "for i in range(1,4):\n",
        "  trans_X = torch.cat((trans_X, X[:, i:i+2].reshape(6, -1)), 1)\n",
        "tmp_c = torch.mm(W,trans_X)+b.reshape(2,-1)\n",
        "relu = torch.nn.ReLU()\n",
        "c = relu(tmp_c)\n",
        "s = torch.max(c,1).values\n",
        "p = sigmoid_tensor(torch.dot(s,v))\n",
        "loss = calc_loss_tensor(p,1)\n",
        "print(loss)\n",
        "loss.backward()\n",
        "W.grad"
      ],
      "metadata": {
        "id": "0mz0yeWF0PIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4baa7a0b-dae3-4113-f028-fb5b76cf5017"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-0.4741, grad_fn=<LogBackward0>)\n",
            "tensor(0.6225, grad_fn=<MulBackward0>)\n",
            "tensor(0.6931, grad_fn=<SubBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1500,  0.0000, -0.1000, -0.0500, -0.0500, -0.1000],\n",
              "        [-0.1000,  0.0000,  0.0000, -0.1000, -0.1000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (20 points)\n",
        "\n",
        "Give names of two datasets that can be used to evaluate the quality of word embeddings, and explain the datasets with the following perspectives.\n",
        "\n",
        "+ Brief explanation of the task for the evaluation.\n",
        "+ Statistics of the dataset (e.g., the number of instances)\n",
        "+ Measure(s) for evaluating the quality"
      ],
      "metadata": {
        "id": "rPWwlW4F0P0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XdNBHor63biK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3 (20 points)\n",
        "\n",
        "Explain two reasons why Transformers are superior to Recurrent Neural Network\n",
        "(RNN) in sequence-to-sequence tasks such as Machine Translation."
      ],
      "metadata": {
        "id": "xYrvAyBW3TxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**１つ目**\n",
        "\n",
        "シーケンス長をn、全単語を表現するためのベクトルの次元をdとすると、Transformerで用いているSelf-Attensionの層ごとの複雑さを計算するのにかかる時間のオーダーは$O(n^2d)$、RNNについては$O(nd^2)$となる。一般に言語処理を行う際には$n<d$であるため、Transformerの方が有利であることがわかる。\n",
        "\n",
        "**２つ目**\n",
        "\n",
        "Transformerはself-attentionをベースとした構造になっており、self-attentionは全ての位置を$O(1)$で結びつけることができる。一方RNNは前の状態から後の状態を計算するために位置を結びつけるためのかかる時間は$O(n)$となる。この点においてもTransformerはRNNよりも優れている。\n"
      ],
      "metadata": {
        "id": "CqfAdU713cYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4 (10 points)\n",
        "\n",
        "Implement the code for using a pre-trained **language** model. Show the code and its output as well as the following information:\n",
        "\n",
        "+ The detail of the pre-trained language model, for example,\n",
        "    + https://huggingface.co/EleutherAI/gpt-j-6B\n",
        "    + https://huggingface.co/rinna/japanese-gpt-1b\n",
        "    + https://huggingface.co/facebook/blenderbot-400M-distill\n",
        "+ The task addressed by the model (e.g., \"text generation\", \"summarization\", \"chatbot\")\n"
      ],
      "metadata": {
        "id": "YYxI86gA3dGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uCaAqhtCGC2I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}